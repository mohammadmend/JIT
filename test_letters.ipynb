{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import ydf\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(len(train.columns)-2,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,364),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(364,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,29),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(32,29)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        logits=self.layers(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class gestureCreator(Dataset):\n",
    "    def __init__(self,dataframe):\n",
    "        self.data=dataframe\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self):\n",
    "        features=self.data.values.astype(float) \n",
    "        features=torch.tensor(features,dtype=torch.float32)\n",
    "        return features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=torch.load('model_f.pt')\n",
    "m.eval()\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=ydf.load_model(\"/Users/mohammadmendahawi/Desktop/JIT/model_tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728336463.161443 3089366 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1728336463.173596 3092199 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1728336463.179370 3092199 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mph=mp.solutions.hands\n",
    "hand=mph.Hands()\n",
    "mpdraw=mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_mapper={\n",
    "    \n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "    3: 'D',\n",
    "    4: 'del',\n",
    "    5: 'E',\n",
    "    6: 'F',\n",
    "    7: 'G',\n",
    "    8: 'H',\n",
    "    9: 'I',\n",
    "    10: 'J',\n",
    "    11: 'K',\n",
    "    12: 'L',\n",
    "    13: 'M',\n",
    "    14: 'N',\n",
    "    15: 'nothing',\n",
    "    16: 'O',\n",
    "    17: 'P',\n",
    "    18: 'Q',\n",
    "    19: 'R',\n",
    "    20: 'S',\n",
    "    21: 'space',\n",
    "    22: 'T',\n",
    "    23: 'U',\n",
    "    24: 'V',\n",
    "    25: 'W',\n",
    "    26: 'X',\n",
    "    27: 'Y',\n",
    "    28: 'Z'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[f'{coord}{i}'for i in range(21) for coord in ['x','y','z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control():\n",
    "    cap=cv.VideoCapture(1)\n",
    "    while True:\n",
    "        ret,frame=cap.read()\n",
    "\n",
    "        cv.flip(frame,1)\n",
    "        frame_rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        result = hand.process(frame_rgb)\n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_index, hand_landmarks in enumerate(result.multi_hand_landmarks):\n",
    "                handedness = result.multi_handedness[hand_index].classification[0].label\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_h.HAND_CONNECTIONS)\n",
    "                land = []\n",
    "                land2=[hand_index]\n",
    "\n",
    "                for lm in hand_landmarks.landmark:\n",
    "\n",
    "                    land.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "                    land2.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "    \n",
    "\n",
    "                df=pd.DataFrame([land2],columns=columns)\n",
    "\n",
    "                pred=m2.predict(df)\n",
    "\n",
    "                \n",
    "\n",
    "                features=torch.tensor(land,dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    out=m(features)\n",
    "\n",
    "                    _,val=torch.max(out,1)\n",
    "\n",
    "                    pred=val.item()\n",
    "\n",
    "                    label=gesture_mapping.get(pred,\"Unknown\")\n",
    "\n",
    "                avg=(out.cpu().numpy()+pred)/2\n",
    "\n",
    "                pred_ensemble=np.argmax(avg,axis=1)[0]\n",
    "\n",
    "                label_ensemble=gesture_mapping.get(pred_ensemble,\"Unkown\")\n",
    "\n",
    "                wrist_pos=hand_landmarks.landmark[0]\n",
    "\n",
    "                h,w,_=frame.shape\n",
    "\n",
    "                wrist_x=int(wrist_pos.x*w)\n",
    "\n",
    "                wrist_y=int(wrist_pos.y*h)\n",
    "\n",
    "                pos=(wrist_x,wrist_y-30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                cv.putText(\n",
    "\n",
    "                    frame, \n",
    "\n",
    "                    f'{label_ensemble}', \n",
    "\n",
    "                    pos, \n",
    "\n",
    "                    cv.FONT_HERSHEY_SIMPLEX, \n",
    "\n",
    "                    1, \n",
    "\n",
    "                    (255, 0, 0), \n",
    "\n",
    "                    2, \n",
    "\n",
    "                    cv.LINE_AA\n",
    "\n",
    "                )\n",
    "\n",
    "                tasks(label_ensemble)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Display the image \n",
    "\n",
    "        cv.imshow('feed', cv.flip(frame, 1))\n",
    "\n",
    "        if cv.waitKey(1) == ord(\"q\"):\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            cv.destroyAllWindows()\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.control()>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
